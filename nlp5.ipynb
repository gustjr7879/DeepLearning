{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어 전처리 패키지 (Text Preprocessing tools for Korean Text)\n",
    "앞에서 공부했던 형태소와 문장 토크나이징 도구인 Konlpy와 KSS와 함께 사용하기 딱 좋다\n",
    "PyKoSpacing 등이 있다.\n",
    "우선 pip install git+https://github.com/haven-jeon/PyKoSpacing.git 를 사용해서 다운로드 받을 수 있다.\n",
    "이 패키지는 띄어쓰기가 되어있지 않은 문장을 띄어쓰기 해준다. 대용량 코퍼스로 학습한 딥러닝 모델을 기반으로 해주기 때문에 정확하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'\n",
    "#이런 문장이 있을 때 우선 띄어쓰기를 제거해주자\n",
    "new_sent = sent.replace(' ','')\n",
    "\n",
    "from pykospacing import Spacing\n",
    "spacing = Spacing()\n",
    "\n",
    "kospacingsent = spacing(new_sent)\n",
    "print(sent)\n",
    "print(kospacingsent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Py-hanspell은 네이버 한글 맞춤법 검사기를 통해서 만들어진 패키지이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked(result=True, original='맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 ', checked='맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지 ', errors=2, words=OrderedDict([('맞춤법', 0), ('틀리면', 0), ('왜', 1), ('안돼?', 1), ('쓰고', 1), ('싶은', 1), ('대로', 1), ('쓰면', 1), ('되지', 1), ('', 0)]), time=0.08986186981201172)\n",
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지 \n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "spelled_sent = spell_checker.check(sent) # 출력결과를 보면 checked에 틀린것을 고친걸 저장시켜놨기 때문에\n",
    "print(spelled_sent)\n",
    "hanspell_sent = spelled_sent.checked # checked로 호출해서 수정된 버전을 출력한다.\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "new_spelled_sent = spell_checker.check(new_sent)\n",
    "hanspell_sent2 = new_spelled_sent.checked\n",
    "print(hanspell_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 hanspell은 띄어쓰기와 맞춤법 둘다 보정되는 것을 확인할 수 있다. \n",
    "다른 패키지로 SOYNLP를 통해서 진행할 수 있다.\n",
    "진행하기 앞서서 기존 형태소 분석기들이 뭐가 문제인지, SOYNLP는 어떤 부분에서 유리한지 정리하자\n",
    "기존 형태소 분석기들은 신조어나 형태소 분석기에 등록되어있지 않은 말들은 제대로 구분하지 못한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에스', '파', '카리나', 'ㄹㅇ', '존예', '돌']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "text = '에스파 카리나 ㄹㅇ 존예돌'\n",
    "print(tokenizer.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 존예돌 하나로 묶여야하는 것, 그리고 에스파 하나로 묶여야하는 것이 분리된 것을 확인할 수 있다.\n",
    "그렇다면 이런 문제를 해결하려면 어떻게 해야할까?\n",
    "특정 문자 시퀀스가 함께 자주 등장하는 빈도가 높고 앞뒤로 조사 또는 완전히 다른 단어들이 나오는 것을 형태소로 판단하는 토크나이저라면 어떨까?\n",
    "이런 토크나이저가 SOYNLP이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2016-10-20.txt', <http.client.HTTPMessage at 0x170dcebc820>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "#soynlp는 기본적으로 학습에 기반한 토크나이저이므로 학습에 필요한 문서를 다운받는다.\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련데이터를 다수의 문서로 분리\n",
    "\n",
    "corpus = DoublespaceLineCorpus('2016-10-20.txt')\n",
    "len(corpus)\n",
    "\n",
    "i = 0\n",
    "for doc in corpus:\n",
    "    if len(doc)>0:\n",
    "        print(doc)\n",
    "    i+=1\n",
    "    if i == 0:\n",
    "        break #일부분 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.202 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "#soynlp는 학습기반 토크나이저이므로 꼭 학습과정을 거쳐야한다.\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soynlp의 응집확률(cohesion probability)\n",
    "응집 확률은 내부 문자열이 얼마나 응집해서 자주 등장하는지 나타낸다. 값이 높을수록 문자열 시퀀스는 하나의 단어로 등장할 확률이 높고 이를 판단해서 토크나이저를 하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08838002913645132\n",
      "0.37891487632839754\n"
     ]
    }
   ],
   "source": [
    "print(word_score_table[\"반포한\"].cohesion_forward)\n",
    "print(word_score_table[\"반포한강공원\"].cohesion_forward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soynlp의 브랜칭 엔트로피(branching entropy)\n",
    "확률분포의 엔트로피값을 사용한다. 주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지 판단하는 척도이다. \n",
    "예를 들어서 '디'가 들어가면 다음 문자를 맞추기 어렵다.\n",
    "'디스'가 들어가면 조금 더 명확해지지만 아직까진 감이 안오고\n",
    "'디스플'이 되면 거의 '디스플레이'이다. 이를 판단할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.68517802819071\n",
      "1.6371694761537934\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(word_score_table['디'].right_branching_entropy)\n",
    "print(word_score_table['디스'].right_branching_entropy)\n",
    "print(word_score_table['디스플'].right_branching_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soynlp L tokenizer \n",
    "한국어는 L토큰 + R토큰 의 형식을 가질 수 있다. \n",
    "공원에 -> 공원 + 에\n",
    "공부하는 -> 공부 + 하는 \n",
    "이런식으로 말이다. 분리기준을 점수로 표현하고 분리가 가장 잘될때를 찾는다.\n",
    "\n",
    "또한 최대 점수 토크나이저는 띄어쓰기가 되어있지 않을때 점수가 높은 글자 시퀀스를 순차적으로 찾아낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soynlp를 통한 반복문자 정제\n",
    "한국어에서는 ㅋㅋㅋㅋ ㅋㅋㅋ ㅋㅋ ㅋㅋㅋㅋㅋㅋㅋㅋ 와 같이 중복되는 경우가 많다. 이를 통합시키면 더 효율적으로 판단할 수 있을 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
