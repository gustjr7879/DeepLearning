{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 단어의 표현방법이 있다.\n",
    "크게 국소표현방법(Local Representation) , 분산표현방법(Distributed Representation)이 있다.\n",
    "국소표현방법은 단어 하나를 바로 매핑하는 것으로 인덱스로 매핑하는 방법이 가장 큰 예시이다.\n",
    "분산표현방법은 단어의 앞 뒤 단어를 기반으로 단어를 정의한다. 즉 뉘앙스를 표현할 수 있게된다.\n",
    "\n",
    "단어의 등장 순서를 고려하지 않고 오로지 빈도수에 의존하는 국소 표현 방법중 예시는 Bag-of-word가 있다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 딥러닝에서 중요하다고 판단되고, 내가 공부했었지만 까먹었던 내용을 정리한다.\n",
    "기울기 폭주(exploding) 소실(vanishing) 문제이다.\n",
    "\n",
    "깊은 인공신경망을 학습시키다보면 역전파 과정에서 입력층에 가까워질 수록 기울기가 점차적으로 작아지는 경향이 있다.\n",
    "입력층에 가까운 층들에서 가중치 업데이트가 잘 되지 않는다면 최적의 모델을 찾기 어렵다.\n",
    "또한 기울기가 점점 커지더니 결국 비정상적으로 큰 값이 되는 경우도 존재한다.\n",
    "이런 문제점은 RNN모델에서 흔히 나타나는 문제이기도 하다.\n",
    "\n",
    "이런 문제를 해결하기 위한 방법은 무엇이 있을까?\n",
    "\n",
    "1. ReLU와 ReLU 변형 (activation function)\n",
    "시그모이드 함수의 경우 절대값이 클수록 기울기가 0에 가까워진다. 이러면 기울기 소실 문제가 발생할 가능성이 높다. \n",
    "이 문제점을 해결하기 위한 방법으로는 activation function을 sigmoid나 tahn 을 쓰는 대신에 ReLU를 사용하는 방법이 있다.\n",
    "은닉층에서는 시그모이드보다 ReLU나 Leaky ReLU를 사용하도록 하자.\n",
    "\n",
    "2. Gradient Clipping\n",
    "이 방법은 내가 두번째 논문을 작성할 때 사용했던 방법이다. 그래디언트 클리핑이란 말 그대로 기울기 값을 자르는 것에 있다. 기울기 폭주를 막기위해서 임계값을 넘지않게 값을 자른다.\n",
    "다시 말해서 커지지 않도록 임계값 크기를 감소시키는 방법이다. \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "와 같이 사용가능하다.\n",
    "\n",
    "3. 가중치 초기화 (weight initialization)\n",
    "같은 모델을 학습시키더라도 가중치가 초기에 어떤 값을 가지는지가 중요하다. 적절한 가중치로 초기화 하는 것이 기울기 소실과 같은 문제를 해결할 수 있다.\n",
    "\n",
    "- Xavier init- (glorot init-)\n",
    "세이비어 초기화라고도 불린다. 이전층의 뉴런갯수와 다음층의 뉴런갯수를 바탕으로 식을 세우며 균등분포나 정규분포를 사용한다.\n",
    "이 방법은 기울기 분산사이에 균형을 맞춰줘서 특정 층이 주목받는 것을 방지해줄 수 있다. \n",
    "하지만 시그모이드나 하이퍼볼릭탄젠트 함수처럼 S자 activation function과는 조합이 좋지만 ReLU와는 성능 조합이 좋지 않다.\n",
    "\n",
    "-He init-\n",
    "이 방법은 이전층의 뉴런갯수를 사용해서 분포를 내지만 다음층의 뉴런갯수는 고려하지 않는다.\n",
    "ReLU계열 함수와 조합이 좋다.\n",
    "\n",
    "4. 배치 정규화 (Batch Normalization)\n",
    "activation function과 가중치 초기화로 해결할 수 있지만, 학습과정중에 언제든지 다시 기울기 폭주나 소실이 발생할 수 있다.\n",
    "배치 정규화는 각 층에 들어가는 입력을 평균과 분산으로 정규화해서 학습을 효율적으로 만드는 방법이다.\n",
    "기존에는 층마다 데이터의 분포가 바뀌지만 (내부 공변량 변화라고 한다) 배치정규화를 사용하면 분표변화를 줄여줘서 해결할 수 있다고 한다. (내부 공변량 변화가 있으면 기울기 폭주나 소실 문제가 발생한다고 주장)\n",
    "배치 단위로 정규화가 진행되며, 활성화 함수를 통과하기 전에 수행된다. \n",
    "기울기 소실 문제를 크게 개선할 수 있고, 가중치 초기화에 덜 민감하게 된다. 또한 더 큰 학습률을 사용가능하게 함으로서 학습속도를 개선할 수 있다. \n",
    "noise를 추가하는 효과 또한 존재하며 실행시간이 살짝 느려질 수 있다. \n",
    "\n",
    "하지만 문제점(한계점) 또한 존재한다.\n",
    "- 미니배치 크기에 의존적이다.\n",
    "- RNN에 적용하기 어렵다 (time step마다 다른 통계치를 가지기 때문이다)\n",
    "\n",
    "5. 층 정규화 (Layer Normalization)\n",
    "배치 정규화는 간단하게 말해서 열끼리 평균과 표준편차를 구하는 방법이라고 한다면 층 정규화는 행별로 평균과 표준편차를 구한다.\n",
    "이는 미니배치 크기에 의존하지 않고 RNN에도 적용할 수 있다고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 훑어보기\n",
    "\n",
    "앞서 예제를 하면서 Keras를 사용했지만 한번 더 정리하고 넘어가도록 하자.\n",
    "\n",
    "1. 전처리(Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 결과 [1, 2, 3, 4, 5, 6, 7]\n",
      "토크나이저 결과 {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer()\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "train_text = \"The earth is an awesome place live\"\n",
    "\n",
    "#단어 집합 생성 (토크나이저 진행)\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "\n",
    "#정수 형태로 표현\n",
    "sub_text = \"The earth is an awesome place live\"\n",
    "sequence = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "\n",
    "print('정수 인코딩 결과',sequence)\n",
    "print('토크나이저 결과',tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding 결과 [[1 2 3]\n",
      " [4 5 6]\n",
      " [0 7 8]]\n"
     ]
    }
   ],
   "source": [
    "#각 샘플의 길이는 다를 수 있다. 모델의 입력으로 사용하려면 길이를 맞춰줘야하므로 pad_sequences를 사용해서 맞춰주도록 하자\n",
    "print('padding 결과',pad_sequences([[1,2,3],[3,4,5,6],[7,8]],maxlen=3,padding='pre'))\n",
    "#maxlen은 패딩의 길이를 얼마나 할 것인가를 나타내고 padding에 pre를 넣으면 앞에 0, post를 넣으면 뒤에 0을 넣는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 임베딩(Word embedding)\n",
    "뒤에서 한번 자세하게 볼건데 워드임베딩이란 텍스트 내 단어들을 밀집 벡터로 만드는 것을 의미한다. 원-핫 인코딩(벡터)는 대부분이 0이고 몇개만 1이였다. 또한 단어벡터들끼리 유사도를 구하기 까다롭다는 단점도 있다.\n",
    "하지만 워드 임베딩은 상대적으로 저차원의 벡터를 가지며 모든 원소의 값이 실수이다. 표현되는 숫자들은 학습된 숫자이다.\n",
    "Embedding()은 단어를 밀집벡터로 만들어주며 인공신경망 용어로 임베딩 층을 만든다고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#위에 실습에서 진행한 것을 사용하도록 하자.\n",
    "padded = pad_sequences([[1,2,3],[3,4,5,6],[7,8]],maxlen=3,padding='pre')\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "vocap_size = len(tokenizer.word_index)+1\n",
    "embedding_dim = 4\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocap_size,embedding_dim,input_length = 3))\n",
    "#이런 느낌으로 선언하고 나중에 fit할 때 패딩된 데이터를 넣으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#모델링(Modeling)\n",
    "#Sequential - 케라스에서 층을 구성하기 위해서 사용한다. Sequential()로 선언 후 add를 통해서 층을 추가한다. \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=3, activation='relu')) #첫번째 인자 출력층, 두번째 인자 입력 뉴런 수 (입력의 차원), 세번째 층 활성화 함수\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#이외로 .compile을 통해서 최적화함수, loss함수를 설정할 수 있고, metric을 뭐 쓸건지 설정할 수도 있다.\n",
    "#추가로 .fit을 통해서 epochs, batch_size, validation_data, validation_split 등을 설정할 수 있다.\n",
    "#validation_data 는 (v_train, v_test)와 같이 데이터를 직접 넣을 수 있고 validation_split은 0.2로 설정하면 train data의 20%를 검증 데이터로 사용하는 의미가 된다. \n",
    "#verbose인자는 학습 중 출력되는 문구를 나타낸다. 0은 출력 x 1은 진행도를 보여주고 2는 미니배치마다 loss값을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential()은 사용하기 간단하지만 복잡한 모델을 설계하기 무리가 있다.\n",
    "따라서 funtional API를 사용해서 모델을 설계해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 3677.3889 - mse: 3677.3889\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 453.5410 - mse: 453.5410\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 56.8699 - mse: 56.8699\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0615 - mse: 8.0615\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0548 - mse: 2.0548\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3147 - mse: 1.3147\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2225 - mse: 1.2225\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2101 - mse: 1.2101\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2075 - mse: 1.2075\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2061 - mse: 1.2061\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2049 - mse: 1.2049\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2037 - mse: 1.2037\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2025 - mse: 1.2025\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2013 - mse: 1.2013\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2002 - mse: 1.2002\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1990 - mse: 1.1990\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1979 - mse: 1.1979\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1968 - mse: 1.1968\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1956 - mse: 1.1956\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1945 - mse: 1.1945\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1934 - mse: 1.1934\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1923 - mse: 1.1923\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1912 - mse: 1.1912\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1902 - mse: 1.1902\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1891 - mse: 1.1891\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1880 - mse: 1.1880\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1870 - mse: 1.1870\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1860 - mse: 1.1860\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1849 - mse: 1.1849\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1839 - mse: 1.1839\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1829 - mse: 1.1829\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1819 - mse: 1.1819\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1809 - mse: 1.1809\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1799 - mse: 1.1799\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1789 - mse: 1.1789\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1779 - mse: 1.1779\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1769 - mse: 1.1769\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1760 - mse: 1.1760\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1750 - mse: 1.1750\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1741 - mse: 1.1741\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1732 - mse: 1.1732\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1722 - mse: 1.1722\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1713 - mse: 1.1713\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1704 - mse: 1.1704\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1695 - mse: 1.1695\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1686 - mse: 1.1686\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1677 - mse: 1.1677\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1668 - mse: 1.1668\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1659 - mse: 1.1659\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1651 - mse: 1.1651\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1642 - mse: 1.1642\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1633 - mse: 1.1633\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1625 - mse: 1.1625\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1617 - mse: 1.1617\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1608 - mse: 1.1608\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1600 - mse: 1.1600\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1592 - mse: 1.1592\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1584 - mse: 1.1584\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1576 - mse: 1.1576\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1568 - mse: 1.1568\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1560 - mse: 1.1560\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1552 - mse: 1.1552\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1544 - mse: 1.1544\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1536 - mse: 1.1536\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1528 - mse: 1.1528\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1521 - mse: 1.1521\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1513 - mse: 1.1513\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1506 - mse: 1.1506\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1498 - mse: 1.1498\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1491 - mse: 1.1491\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1484 - mse: 1.1484\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1476 - mse: 1.1476\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1469 - mse: 1.1469\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1462 - mse: 1.1462\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1455 - mse: 1.1455\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1448 - mse: 1.1448\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1441 - mse: 1.1441\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1434 - mse: 1.1434\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1427 - mse: 1.1427\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1420 - mse: 1.1420\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1414 - mse: 1.1414\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1407 - mse: 1.1407\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1400 - mse: 1.1400\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1394 - mse: 1.1394\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1387 - mse: 1.1387\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1381 - mse: 1.1381\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1374 - mse: 1.1374\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1368 - mse: 1.1368\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1362 - mse: 1.1362\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1356 - mse: 1.1356\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1349 - mse: 1.1349\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1343 - mse: 1.1343\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1337 - mse: 1.1337\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1331 - mse: 1.1331\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1325 - mse: 1.1325\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1319 - mse: 1.1319\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1313 - mse: 1.1313\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1307 - mse: 1.1307\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1301 - mse: 1.1301\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1296 - mse: 1.1296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8da958880>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "X = [1,2,3,4,5,6,7,8,9]\n",
    "Y = [11,22,33,44,53,66,77,87,95]\n",
    "\n",
    "input = Input(shape=(1,))\n",
    "output = Dense(1,activation='linear')(input)\n",
    "linear_model = Model(input,output)\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "linear_model.compile(optimizer=sgd,loss='mse',metrics=['mse'])\n",
    "linear_model.fit(X,Y,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[54.339893]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict([5])\n",
    "#5시간 하면 54.34점을 받는다고 예측하고 있다. 나름 정확하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
