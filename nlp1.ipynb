{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치의 nn.RNN으로 RNN 간단하게 구현해보기\n",
    "\n",
    "RNN(순환 신경망)은 시퀀스 모델이다. 데이터가 한번에 다 들어가는 것이 아닌 시퀀스 단위로 처리하는 모델이다. \n",
    "시퀀스로 처리하게 되면 예를 들어서 \"나는 정말 피곤하다.\" 라는 문장이 있을 때\n",
    "1. 나는\n",
    "2. 정말\n",
    "3. 피곤하다\n",
    "순서로 처리하게 되는데 이때 나는에 사용되는 가중치 W와 정말에 사용되는 W, 피곤하다에 사용되는 W가 다르게 된다. \n",
    "\n",
    "대략적으로 순서는 나는 정말 피곤하다를 각각 embedding해서 벡터로 표현하고, 이를 hidden layer에서 순차적으로 처리한다.\n",
    "embedding(나는) * W ->  embedding(정말) * W + embedding(나는) * W --- \n",
    "이런식으로 진행된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "#입력 텐서 정의 : (배치크기 * 시점의 수 * 매 시점마다 들어가는 입력)\n",
    "inputs = torch.Tensor(1, 10 , 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.1324e-14, 2.0893e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
      "tensor([[[-0.4847, -0.1222,  0.5851,  0.0393,  0.2962, -0.3598, -0.0980,\n",
      "           0.1506],\n",
      "         [-0.6468, -0.1595,  0.3162, -0.1091,  0.4427, -0.3270, -0.1337,\n",
      "          -0.1943],\n",
      "         [-0.6665, -0.1499,  0.2470, -0.2614,  0.4655, -0.4559, -0.3292,\n",
      "          -0.1864],\n",
      "         [-0.6651, -0.1657,  0.2481, -0.3438,  0.5003, -0.4393, -0.3640,\n",
      "          -0.2329],\n",
      "         [-0.6824, -0.1867,  0.2314, -0.3484,  0.5074, -0.4347, -0.3714,\n",
      "          -0.2442],\n",
      "         [-0.6862, -0.1878,  0.2240, -0.3607,  0.5020, -0.4419, -0.3820,\n",
      "          -0.2502],\n",
      "         [-0.6877, -0.1874,  0.2255, -0.3664,  0.5047, -0.4442, -0.3845,\n",
      "          -0.2538],\n",
      "         [-0.6889, -0.1882,  0.2239, -0.3675,  0.5067, -0.4442, -0.3852,\n",
      "          -0.2554],\n",
      "         [-0.6890, -0.1885,  0.2231, -0.3687,  0.5067, -0.4447, -0.3865,\n",
      "          -0.2556],\n",
      "         [-0.6891, -0.1886,  0.2232, -0.3692,  0.5068, -0.4447, -0.3867,\n",
      "          -0.2558]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size,hidden_size,batch_first = True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell은 2개의 값을 return하게 되는데 첫번째는 모든 시점의 은닉상태들이고, 두번째는 마지막 시점의 은닉상태이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델은 깊게 쌓을 수 있다. 다음 time 뿐만 아니라 다음 layer에도 전달하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0065, -0.1013, -0.4842, -0.1214,  0.2630, -0.1605,  0.3212,\n",
      "           0.1009],\n",
      "         [-0.0424,  0.0148, -0.6516, -0.0543,  0.0528,  0.0638,  0.4792,\n",
      "           0.2978],\n",
      "         [ 0.1468, -0.0850, -0.7082,  0.0095, -0.0115,  0.1223,  0.4859,\n",
      "           0.2777],\n",
      "         [ 0.1072, -0.0542, -0.6750,  0.0455, -0.0238,  0.2401,  0.4979,\n",
      "           0.2571],\n",
      "         [ 0.1151, -0.0680, -0.6744,  0.0115, -0.0368,  0.2548,  0.5083,\n",
      "           0.2826],\n",
      "         [ 0.1227, -0.0807, -0.6739,  0.0095, -0.0303,  0.2497,  0.5152,\n",
      "           0.2898],\n",
      "         [ 0.1172, -0.0745, -0.6729,  0.0108, -0.0336,  0.2551,  0.5172,\n",
      "           0.2905],\n",
      "         [ 0.1210, -0.0765, -0.6736,  0.0105, -0.0331,  0.2537,  0.5170,\n",
      "           0.2914],\n",
      "         [ 0.1198, -0.0756, -0.6733,  0.0115, -0.0334,  0.2549,  0.5170,\n",
      "           0.2908],\n",
      "         [ 0.1203, -0.0757, -0.6733,  0.0112, -0.0334,  0.2549,  0.5171,\n",
      "           0.2910]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1,10,5) # batch time inputsize\n",
    "cell = nn.RNN(input_size=5, hidden_size=8,num_layers=2, batch_first=True)\n",
    "\n",
    "output,_status = cell(inputs)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "양방향 순환 신경망은 언어라는 것이 이전 시점 데이터 뿐만 아니라 미래에 있는 데이터 또한 불러와야 되기 때문에 필요하다.\n",
    "RNN이 과거 시점의 데이터들을 참고해서 정답을 예측한다. 또한 향후에 시점의 데이터들도 참고한다.\n",
    "두개의 메모리 셀을 두고 하나는 기본 RNN과 같이 다음 time step으로 보내고, 하나는 다음 time step에서 가져온다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0774, -0.1980,  0.3144,  0.2494,  0.3557, -0.1459,  0.4364,\n",
      "          -0.5083,  0.0631, -0.4325, -0.4235,  0.4349,  0.2658, -0.0540,\n",
      "           0.1977,  0.1565],\n",
      "         [ 0.3184, -0.2619,  0.4942,  0.6167,  0.2611, -0.3365,  0.3204,\n",
      "          -0.5484,  0.1817, -0.3334, -0.3389,  0.4242,  0.2895, -0.1975,\n",
      "           0.1963,  0.1367],\n",
      "         [ 0.3915, -0.4621,  0.4402,  0.5916,  0.2099, -0.3588,  0.1682,\n",
      "          -0.6453,  0.1990, -0.3000, -0.3420,  0.4382,  0.2982, -0.1531,\n",
      "           0.1744,  0.1401],\n",
      "         [ 0.3322, -0.5013,  0.4158,  0.5072,  0.2760, -0.3235,  0.1136,\n",
      "          -0.6745,  0.1874, -0.3216, -0.3487,  0.4357,  0.2803, -0.1541,\n",
      "           0.1612,  0.1480],\n",
      "         [ 0.3164, -0.4890,  0.4210,  0.4936,  0.2956, -0.3179,  0.1505,\n",
      "          -0.6673,  0.1965, -0.3081, -0.3534,  0.4250,  0.2859, -0.1619,\n",
      "           0.1661,  0.1486],\n",
      "         [ 0.3296, -0.4743,  0.4304,  0.5139,  0.2858, -0.3165,  0.1575,\n",
      "          -0.6633,  0.1879, -0.3133, -0.3414,  0.4277,  0.2931, -0.1509,\n",
      "           0.1627,  0.1290],\n",
      "         [ 0.3345, -0.4785,  0.4350,  0.5176,  0.2794, -0.3221,  0.1505,\n",
      "          -0.6677,  0.2123, -0.3387, -0.3457,  0.4550,  0.2801, -0.1653,\n",
      "           0.1738,  0.1643],\n",
      "         [ 0.3549, -0.4861,  0.4122,  0.5223,  0.2673, -0.3206,  0.1461,\n",
      "          -0.6536,  0.2051, -0.2499, -0.3661,  0.4203,  0.2959, -0.1997,\n",
      "           0.1261,  0.1709],\n",
      "         [ 0.3334, -0.4826,  0.4422,  0.5228,  0.2382, -0.3188,  0.1221,\n",
      "          -0.6819,  0.1542, -0.3331, -0.2987,  0.4634,  0.3138, -0.0805,\n",
      "           0.1019,  0.1092],\n",
      "         [ 0.4939, -0.5027,  0.3242,  0.6105,  0.1452, -0.3569,  0.1142,\n",
      "          -0.5960,  0.1398, -0.2555, -0.2791,  0.3490,  0.1531, -0.1580,\n",
      "           0.0742,  0.3093]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1,10,5)\n",
    "cell = nn.RNN(5,8,2,batch_first=True, bidirectional=True) #bidirectional 을 True로 해주면 양방향이 적용된다.\n",
    "output,_status = cell(inputs)\n",
    "print(output)\n",
    "#이때 output의 size는 2배가 된다 (미래로 보내는것과 미래에서 가져오는애들이 concat되어있음)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
