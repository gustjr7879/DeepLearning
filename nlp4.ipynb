{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding (패딩)\n",
    "자연어 처리를 하다보면 각 문장의 길이는 다를 수 있다. 하지만 기계가 처리하는 방식은 행렬을 기준으로 처리하기 때문에 행렬연산을 위해서 패딩을 해줌으로서 길이를 맞추는 과정이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "sentence 최대길이\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)\n",
    "\n",
    "maxlen = max(len(item) for item in encoded)\n",
    "print('sentence 최대길이')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "for sentence in encoded:\n",
    "    while len(sentence) < maxlen:\n",
    "        sentence.append(0)\n",
    "\n",
    "pad_np = np.array(encoded)\n",
    "print(pad_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 하면 기계가 병렬처리를 수월하게 진행할 수 있다. 0으로 채우는 것을 zero-padding이라고 한다.\n",
    "케라스에서는 위와 같은 패딩처리를 위해서 pad_sequences()를 제공하고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  1  8  5]\n",
      " [ 0  0  0  0  1  3  5]\n",
      " [ 0  0  0  0  0  9  2]\n",
      " [ 0  0  0  2  4  3  2]\n",
      " [ 0  0  0  0  0  3  2]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  2]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 0  0  0  1 12  3 13]]\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padd = pad_sequences(encoded) #0이 앞으로 가게\n",
    "\n",
    "print(padd)\n",
    "padd = pad_sequences(encoded, padding='post') #0이 뒤로가게\n",
    "print(padd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제에서는 길이를 가장 긴 것에 맞춰놨지만, 모든 경우에서 가장 긴것에 맞추는건 아니다.\n",
    "다음과 같이 맞춤 길이를 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n",
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 7  7  3  2 10]\n",
      " [ 1 12  3 13  0]]\n"
     ]
    }
   ],
   "source": [
    "padd = pad_sequences(encoded,padding='post',maxlen=5) #이렇게 하면 앞에 두자리가 삭제되는 것을 볼 수 있다. \n",
    "print(padd)\n",
    "padd = pad_sequences(encoded,padding='post',truncating='post',maxlen=5) #이렇게 하면 뒤에 두자리가 삭제되는 것을 볼 수 있다. 추가로 value = '값' 을 설정해서 0대신 사용할 값을 설정해줄 수 있다.\n",
    "print(padd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩(One-Hot encoding)\n",
    "원핫인코딩을 배우기 앞서 단어 집합을 정의해보자 단어 집합은 서로 다른 단어들의 집합을 나타낸다. \n",
    "원핫인코딩을 하기위해서 단어집합을 만들어야한다. 다음에 단어 집합의 크기를 벡터의 차원으로 설정하고 그 인덱스에 1, 다른 인덱스에 0으로 표현하는 기법이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "token = okt.morphs('나는 자연어 처리를 배운다')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n",
      "[0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word:index for index, word in enumerate(token)}\n",
    "print(word_to_index)\n",
    "\n",
    "def one_hot(word,word_to_index):\n",
    "    one_hot_vec = [0]*len(word_to_index)\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vec[index] = 1\n",
    "    return one_hot_vec\n",
    "\n",
    "ohe = one_hot('자연어',word_to_index)\n",
    "print(ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 keras를 이용한 원핫인코딩을 보여준다. to_categorical 을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 토크나이저 결과 :  {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print('keras 토크나이저 결과 : ',tokenizer.word_index)\n",
    "#이렇게 토크나이저 한 것을 토대로 정제했다고 치고 sub_text를 만든 뒤 순서로만 나타내려면 texts_to_sequences를 사용한다.\n",
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)\n",
    "\n",
    "ohe = to_categorical(encoded) #그리고 to_categorical 을 사용해서 원핫인코딩으로 만들 수 있다.\n",
    "print(ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 원핫인코딩을 사용하면 단어들의 유사성을 잃어버릴 수 있고 저장공간에 매우 비효율적이다.\n",
    "따라서 단어의 잠재의미를 파악해서 벡터화하는 카운트기반 방법인 LSA(잠재의미분석), HAL 등이 있고 예측을 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText등이 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
